module NeuralNetwork

using ..AutoDiff # Dostp do TrackedValue, backward!, ops etc.
# Jeli operacje z AutoDiff.ops nie s eksportowane globalnie, trzeba je zaimportowa:
# using ..AutoDiff.OpsAD: relu, matmul, +, * # etc. lub using ..AutoDiff: relu, matmul
# Za贸偶my, 偶e s dostpne przez AutoDiff.relu_op, AutoDiff.matmul_op lub przeci偶one operatory

# println(" wasna biblioteka NeuralNetwork zaadowana! ")

# Definicje warstw
include("layers.jl")
export Dense, Chain, ReLU # ReLU jako warstwa/obiekt aktywacji

# Funkcje kosztu
include("loss.jl")
export mse_loss

# Optymalizatory
include("optimizers.jl")
export SGD, update! # update! to funkcja do aktualizacji wag

# Logika treningu
include("train.jl")
export train!

# Funkcja pomocnicza do zbierania parametr贸w modelu (TrackedValue)
function params(layer) # Generyczna funkcja, specjalizacje w layers.jl
    # Domylnie warstwa nie ma parametr贸w
    return []
end

function params(model_or_layer_array::Union{Chain, AbstractArray})
    all_params = TrackedValue[]
    for layer in (isa(model_or_layer_array, Chain) ? model_or_layer_array.layers : model_or_layer_array)
        append!(all_params, params(layer))
    end
    return all_params
end

# Funkcja do zerowania gradient贸w wszystkich parametr贸w w modelu
function zero_grad_model!(model_or_layer_collection)
    ps = params(model_or_layer_collection)
    for p in ps
        zero_grad!(p) # U偶ywa zero_grad! z AutoDiff.core
    end
end


end # module NeuralNetwork